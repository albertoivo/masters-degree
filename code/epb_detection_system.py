"""
Sistema Completo de Reconhecimento de EPBs usando 2DPCA
Autor: Sistema de ML para Detec√ß√£o de Bolhas de Plasma Equatorial
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from PIL import Image

# Sklearn imports
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (confusion_matrix, roc_curve, auc, precision_recall_curve)
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# Processamento de imagem
from scipy.ndimage import gaussian_filter, rotate, shift


# ============================================================================
# PARTE 1: PR√â-PROCESSAMENTO E CARREGAMENTO DE DADOS
# ============================================================================

def preprocess_epb_image(img, apply_background_removal=True, enhance_contrast=True):
    """
    Pr√©-processamento espec√≠fico para real√ßar estruturas de EPB
    
    Args:
        img: Imagem em escala de cinza [0, 255] ou [0, 1]
        apply_background_removal: Remove gradientes de fundo
        enhance_contrast: Aplica CLAHE para real√ßar contraste local
    
    Returns:
        Imagem pr√©-processada normalizada [0, 1]
    """
    # Garante [0, 1]
    if img.max() > 1.0:
        img = img / 255.0
    
    img = img.astype(np.float32)
    
    # 1. Remove gradientes de fundo (ilumina√ß√£o n√£o-uniforme)
    if apply_background_removal:
        # Estima background com filtro gaussiano
        background = gaussian_filter(img, sigma=10)
        img = img - background
        # Garante valores positivos
        img = np.clip(img, 0, None)
    
    # 2. Realce de contraste (similar a CLAHE)
    if enhance_contrast:
        # Equaliza√ß√£o adaptativa por blocos
        from skimage.exposure import equalize_adapthist
        img = equalize_adapthist(img, clip_limit=0.03)
    
    # 3. Normaliza√ß√£o final
    if img.max() > img.min():
        img = (img - img.min()) / (img.max() - img.min())
    
    return img


def load_epb_dataset(data_folder, label_file, target_size=(64, 64), apply_preprocessing=True):
    """
    Carrega dataset de imagens de EPB
    
    Estrutura esperada:
        data_folder/
            ‚îú‚îÄ‚îÄ image_001.png
            ‚îú‚îÄ‚îÄ image_002.png
            ‚îî‚îÄ‚îÄ ...
        
        label_file.csv:
            filename,has_epb
            image_001.png,1
            image_002.png,0
            ...
    
    Args:
        data_folder: Pasta com as imagens
        label_file: CSV com colunas [filename, has_epb]
        target_size: Tamanho para redimensionar imagens
        apply_preprocessing: Aplica pr√©-processamento especializado
    
    Returns:
        images: array (n_samples, height, width)
        labels: array (n_samples,)
        filenames: lista de nomes dos arquivos
    """
    print("üìÇ Carregando dataset de EPBs...")
    
    # L√™ arquivo de labels
    df = pd.read_csv(label_file)
    print(f"   ‚úì Labels carregados: {len(df)} registros")
    
    images = []
    labels = []
    filenames = []
    errors = []
    
    for idx, row in df.iterrows():
        img_path = Path(data_folder) / row['filename']
        
        try:
            # Carrega imagem
            img = Image.open(img_path).convert('L')  # Grayscale
            img = np.array(img.resize(target_size))
            
            # Aplica pr√©-processamento
            if apply_preprocessing:
                img = preprocess_epb_image(img)
            else:
                img = img / 255.0  # Apenas normaliza
            
            images.append(img)
            labels.append(int(row['has_epb']))
            filenames.append(row['filename'])
            
        except Exception as e:
            errors.append((row['filename'], str(e)))
    
    if errors:
        print(f"   ‚ö†Ô∏è  {len(errors)} imagens com erro:")
        for fname, error in errors[:5]:  # Mostra primeiros 5
            print(f"      ‚Ä¢ {fname}: {error}")
    
    images = np.array(images)
    labels = np.array(labels)
    
    print(f"   ‚úì {len(images)} imagens carregadas com sucesso")
    print(f"   ‚úì Dimens√£o das imagens: {images.shape[1]}x{images.shape[2]}")
    
    # Estat√≠sticas do dataset
    n_epb = np.sum(labels)
    n_no_epb = len(labels) - n_epb
    print(f"\nüìä Distribui√ß√£o das classes:")
    print(f"   ‚Ä¢ EPBs detectados: {n_epb} ({n_epb/len(labels)*100:.1f}%)")
    print(f"   ‚Ä¢ Sem EPB: {n_no_epb} ({n_no_epb/len(labels)*100:.1f}%)")
    
    if n_epb / len(labels) < 0.3 or n_epb / len(labels) > 0.7:
        print(f"   ‚ö†Ô∏è  Dataset desbalanceado! Ser√° aplicado balanceamento.")
    
    return images, labels, filenames


def augment_epb_data(images, labels, augment_factor=2):
    """
    Data Augmentation para aumentar dataset
    
    Aplica transforma√ß√µes geom√©tricas preservando caracter√≠sticas de EPB
    
    Args:
        images: array (n_samples, height, width)
        labels: array (n_samples,)
        augment_factor: Fator de multiplica√ß√£o do dataset
    
    Returns:
        images_aug: array aumentado
        labels_aug: array de labels aumentado
    """
    print(f"\nüîÑ Aplicando Data Augmentation (fator={augment_factor})...")
    
    augmented_images = list(images)
    augmented_labels = list(labels)
    
    for img, label in zip(images, labels):
        for _ in range(augment_factor - 1):
            # Escolhe transforma√ß√£o aleat√≥ria
            aug_type = np.random.choice(['flip', 'rotate', 'shift', 'noise'])
            
            if aug_type == 'flip':
                # Flip horizontal (EPBs podem aparecer em qualquer longitude)
                aug_img = np.fliplr(img)
            
            elif aug_type == 'rotate':
                # Rota√ß√£o leve (-10 a 10 graus)
                angle = np.random.uniform(-10, 10)
                aug_img = rotate(img, angle, reshape=False, mode='constant')
            
            elif aug_type == 'shift':
                # Deslocamento leve
                shift_x = np.random.randint(-5, 5)
                shift_y = np.random.randint(-5, 5)
                aug_img = shift(img, [shift_y, shift_x], mode='constant')
            
            elif aug_type == 'noise':
                # Adiciona ru√≠do gaussiano
                noise = np.random.normal(0, 0.02, img.shape)
                aug_img = img + noise
                aug_img = np.clip(aug_img, 0, 1)
            
            augmented_images.append(aug_img)
            augmented_labels.append(label)
    
    augmented_images = np.array(augmented_images)
    augmented_labels = np.array(augmented_labels)
    
    print(f"   ‚úì Dataset expandido: {len(images)} ‚Üí {len(augmented_images)} imagens")
    
    return augmented_images, augmented_labels


# ============================================================================
# PARTE 2: 2DPCA E EXTRA√á√ÉO DE FEATURES
# ============================================================================

class TwoDPCA:
    """2DPCA para extra√ß√£o de features de imagens de EPB"""
    
    def __init__(self, n_components=10):
        self.n_components = n_components
        self.projection_matrix = None
        self.mean_image = None
        self.explained_variance = None
        self.explained_variance_ratio = None
    
    def fit(self, images):
        """Treina o 2DPCA"""
        n_samples, height, width = images.shape
        
        self.mean_image = np.mean(images, axis=0)
        centered_images = images - self.mean_image
        
        # Matriz de covari√¢ncia
        covariance_matrix = np.zeros((width, width))
        for img in centered_images:
            covariance_matrix += img.T @ img
        covariance_matrix /= n_samples
        
        # Autovalores e autovetores
        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)
        
        # Ordena em ordem decrescente
        idx = eigenvalues.argsort()[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        
        # Seleciona top n_components
        self.projection_matrix = eigenvectors[:, :self.n_components]
        self.explained_variance = eigenvalues[:self.n_components]
        
        total_variance = np.sum(eigenvalues)
        self.explained_variance_ratio = self.explained_variance / total_variance
        
        return self
    
    def transform(self, images):
        """Projeta imagens no espa√ßo reduzido"""
        centered_images = images - self.mean_image
        projected = np.array([img @ self.projection_matrix 
                             for img in centered_images])
        return projected
    
    def fit_transform(self, images):
        return self.fit(images).transform(images)
    
    def inverse_transform(self, projected_images):
        """Reconstr√≥i imagens (√∫til para visualiza√ß√£o)"""
        reconstructed = np.array([proj @ self.projection_matrix.T 
                                 for proj in projected_images])
        return reconstructed + self.mean_image


def optimize_n_components(X_train, y_train, max_components=30, cv_folds=5):
    """
    Otimiza n√∫mero de componentes 2DPCA via valida√ß√£o cruzada
    
    Testa diferentes valores e retorna o melhor baseado em accuracy
    """
    print(f"\nüî¨ Otimizando n√∫mero de componentes (m√°x={max_components})...")
    
    results = []
    test_range = range(5, min(max_components + 1, X_train.shape[2]), 5)
    
    for n_comp in test_range:
        # Treina 2DPCA
        tdpca = TwoDPCA(n_components=n_comp)
        tdpca.fit(X_train)
        
        # Extrai features
        features = tdpca.transform(X_train)
        features = features.reshape(features.shape[0], -1)
        
        # Normaliza
        scaler = StandardScaler()
        features = scaler.fit_transform(features)
        
        # Valida√ß√£o cruzada com SVM
        clf = SVC(kernel='rbf', gamma='scale', class_weight='balanced')
        scores = cross_val_score(clf, features, y_train, 
                                cv=StratifiedKFold(cv_folds), 
                                scoring='f1')
        
        mean_score = scores.mean()
        std_score = scores.std()
        
        # Calcula vari√¢ncia explicada
        var_explained = np.sum(tdpca.explained_variance_ratio)
        
        results.append({
            'n_components': n_comp,
            'f1_score': mean_score,
            'f1_std': std_score,
            'variance_explained': var_explained
        })
        
        print(f"   ‚Ä¢ {n_comp:2d} comp: F1={mean_score:.4f}¬±{std_score:.4f} | "
              f"Var={var_explained:.2%}")
    
    # Encontra melhor configura√ß√£o
    best_result = max(results, key=lambda x: x['f1_score'])
    best_n = best_result['n_components']
    
    print(f"\nüèÜ Melhor configura√ß√£o: {best_n} componentes")
    print(f"   ‚Ä¢ F1-Score: {best_result['f1_score']:.4f}")
    print(f"   ‚Ä¢ Vari√¢ncia explicada: {best_result['variance_explained']:.2%}")
    
    return best_n, results


# ============================================================================
# PARTE 3: SISTEMA COMPLETO COM ENSEMBLE E BALANCEAMENTO
# ============================================================================

class EPBRecognitionSystem:
    """
    Sistema completo de reconhecimento de EPBs
    - 2DPCA para extra√ß√£o de features
    - Balanceamento de classes (SMOTE)
    - Ensemble de classificadores
    """
    
    def __init__(self, n_components=15, use_ensemble=True, 
                 balance_data=True, balance_method='smote'):
        """
        Args:
            n_components: N√∫mero de componentes 2DPCA
            use_ensemble: Usar ensemble ou SVM √∫nico
            balance_data: Aplicar balanceamento de classes
            balance_method: 'smote', 'undersample', ou 'combined'
        """
        self.n_components = n_components
        self.use_ensemble = use_ensemble
        self.balance_data = balance_data
        self.balance_method = balance_method
        
        self.tdpca = TwoDPCA(n_components=n_components)
        self.scaler = StandardScaler()
        self.sampler = None
        
        # Configura classificador
        if use_ensemble:
            self.classifier = self._create_ensemble()
        else:
            self.classifier = SVC(kernel='rbf', probability=True, 
                                 gamma='scale', C=1.0, 
                                 class_weight='balanced', random_state=42)
    
    def _create_ensemble(self):
        """Cria ensemble de classificadores"""
        estimators = [
            ('svm_rbf', SVC(kernel='rbf', probability=True, gamma='scale', 
                           C=1.0, class_weight='balanced', random_state=42)),
            ('svm_poly', SVC(kernel='poly', degree=3, probability=True, 
                            class_weight='balanced', random_state=42)),
            ('rf', RandomForestClassifier(n_estimators=100, max_depth=10,
                                         class_weight='balanced', random_state=42))
        ]
        
        return VotingClassifier(estimators=estimators, voting='soft')
    
    def _setup_balancing(self, y_train):
        """Configura m√©todo de balanceamento"""
        if not self.balance_data:
            return None
        
        if self.balance_method == 'smote':
            # SMOTE: sobreamostragem sint√©tica
            return SMOTE(random_state=42)
        
        elif self.balance_method == 'undersample':
            # Subamostragem da classe majorit√°ria
            return RandomUnderSampler(random_state=42)
        
        elif self.balance_method == 'combined':
            # Combina ambos
            from imblearn.combine import SMOTEENN
            return SMOTEENN(random_state=42)
    
    def extract_features(self, images):
        """Extrai features usando 2DPCA"""
        projected = self.tdpca.transform(images)
        features = projected.reshape(projected.shape[0], -1)
        return features
    
    def fit(self, X_train, y_train):
        """Treina o sistema completo"""
        print("\nüîπ Iniciando treinamento do sistema...")
        
        # 1. Treina 2DPCA
        print("   ‚Ä¢ Treinando 2DPCA para extra√ß√£o de features...")
        self.tdpca.fit(X_train)
        var_explained = np.sum(self.tdpca.explained_variance_ratio)
        print(f"     ‚úì Vari√¢ncia explicada: {var_explained:.2%}")
        
        # 2. Extrai features
        print("   ‚Ä¢ Extraindo features...")
        features_train = self.extract_features(X_train)
        print(f"     ‚úì Shape das features: {features_train.shape}")
        
        # 3. Normaliza
        print("   ‚Ä¢ Normalizando features...")
        features_train = self.scaler.fit_transform(features_train)
        
        # 4. Balanceia dados se necess√°rio
        if self.balance_data:
            print(f"   ‚Ä¢ Aplicando balanceamento ({self.balance_method})...")
            original_dist = np.bincount(y_train)
            
            self.sampler = self._setup_balancing(y_train)
            features_train, y_train = self.sampler.fit_resample(
                features_train, y_train
            )
            
            new_dist = np.bincount(y_train)
            print(f"     ‚úì Classe 0: {original_dist[0]} ‚Üí {new_dist[0]}")
            print(f"     ‚úì Classe 1: {original_dist[1]} ‚Üí {new_dist[1]}")
        
        # 5. Treina classificador
        clf_type = "Ensemble" if self.use_ensemble else "SVM"
        print(f"   ‚Ä¢ Treinando classificador {clf_type}...")
        self.classifier.fit(features_train, y_train)
        print("     ‚úì Treinamento conclu√≠do!")
        
        return self
    
    def predict(self, X_test):
        """Prediz se h√° EPB"""
        features_test = self.extract_features(X_test)
        features_test = self.scaler.transform(features_test)
        return self.classifier.predict(features_test)
    
    def predict_proba(self, X_test):
        """Retorna probabilidades"""
        features_test = self.extract_features(X_test)
        features_test = self.scaler.transform(features_test)
        return self.classifier.predict_proba(features_test)
    
    def get_feature_importance(self, X_sample):
        """
        Analisa quais regi√µes da imagem s√£o mais importantes
        
        Usa gradiente das features em rela√ß√£o √† imagem original
        """
        # Extrai features
        features = self.extract_features(X_sample.reshape(1, *X_sample.shape))
        
        # Reconstr√≥i imagem projetada
        projected = self.tdpca.transform(X_sample.reshape(1, *X_sample.shape))
        reconstructed = self.tdpca.inverse_transform(projected)[0]
        
        # Diferen√ßa mostra import√¢ncia
        importance_map = np.abs(X_sample - reconstructed)
        
        return importance_map, reconstructed


# ============================================================================
# PARTE 4: VISUALIZA√á√ÉO E INTERPRETA√á√ÉO
# ============================================================================

def plot_comprehensive_analysis(system, X_train, X_test, y_train, y_test, 
                                y_pred, y_proba, filenames_test=None):
    """An√°lise visual completa do sistema"""
    
    fig = plt.figure(figsize=(20, 16))
    gs = fig.add_gridspec(5, 6, hspace=1, wspace=0.4)
    
    # ========== ROW 1: EXEMPLOS DE CLASSIFICA√á√ÉO ==========
    for i in range(6):
        ax = fig.add_subplot(gs[0, i])
        ax.imshow(X_test[i], cmap='plasma', aspect='auto')
        
        true_label = "EPB" if y_test[i] == 1 else "Sem EPB"
        pred_label = "EPB" if y_pred[i] == 1 else "Sem EPB"
        prob = y_proba[i, 1] * 100
        
        color = 'green' if y_test[i] == y_pred[i] else 'red'
        title = f'Real: {true_label}\nPred: {pred_label} ({prob:.0f}%)'
        if filenames_test:
            title = f"{filenames_test[i][:15]}...\n" + title
        
        ax.set_title(title, fontsize=8, color=color)
        ax.axis('off')
    
    # ========== ROW 2: AN√ÅLISE DE FEATURES ==========
    # Vari√¢ncia por componente
    ax = fig.add_subplot(gs[1, 0:2])
    variance_ratio = system.tdpca.explained_variance_ratio
    ax.bar(range(len(variance_ratio)), variance_ratio, color='steelblue', alpha=0.7)
    ax.set_xlabel('Componente', fontsize=10)
    ax.set_ylabel('Vari√¢ncia Explicada', fontsize=10)
    ax.set_title('Import√¢ncia de Cada Componente 2DPCA', fontsize=11, fontweight='bold')
    ax.grid(alpha=0.3, axis='y')
    
    # Vari√¢ncia cumulativa
    ax = fig.add_subplot(gs[1, 2:4])
    cumulative = np.cumsum(variance_ratio)
    ax.plot(cumulative, 'o-', color='steelblue', linewidth=2)
    ax.axhline(y=0.9, color='r', linestyle='--', alpha=0.7, label='90%')
    ax.axhline(y=0.95, color='orange', linestyle='--', alpha=0.7, label='95%')
    ax.set_xlabel('N√∫mero de Componentes', fontsize=10)
    ax.set_ylabel('Vari√¢ncia Acumulada', fontsize=10)
    ax.set_title('Vari√¢ncia Cumulativa Explicada', fontsize=11, fontweight='bold')
    ax.legend()
    ax.grid(alpha=0.3)
    
    # Matriz de confus√£o
    ax = fig.add_subplot(gs[1, 4:6])
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Sem EPB', 'EPB'],
                yticklabels=['Sem EPB', 'EPB'], ax=ax, 
                cbar_kws={'label': 'Contagem'})
    ax.set_title('Matriz de Confus√£o', fontsize=11, fontweight='bold')
    ax.set_ylabel('Classe Real', fontsize=10)
    ax.set_xlabel('Classe Predita', fontsize=10)
    
    # ========== ROW 3: M√âTRICAS DE PERFORMANCE ==========
    # Curva ROC
    ax = fig.add_subplot(gs[2, 0:2])
    fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])
    roc_auc = auc(fpr, tpr)
    ax.plot(fpr, tpr, color='darkorange', lw=2, 
            label=f'ROC (AUC = {roc_auc:.3f})')
    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
            label='Classificador Aleat√≥rio')
    ax.set_xlabel('Taxa de Falso Positivo', fontsize=10)
    ax.set_ylabel('Taxa de Verdadeiro Positivo', fontsize=10)
    ax.set_title('Curva ROC', fontsize=11, fontweight='bold')
    ax.legend(loc="lower right")
    ax.grid(alpha=0.3)
    
    # Curva Precision-Recall
    ax = fig.add_subplot(gs[2, 2:4])
    precision, recall, _ = precision_recall_curve(y_test, y_proba[:, 1])
    pr_auc = auc(recall, precision)
    ax.plot(recall, precision, color='purple', lw=2,
            label=f'PR (AUC = {pr_auc:.3f})')
    ax.set_xlabel('Recall', fontsize=10)
    ax.set_ylabel('Precision', fontsize=10)
    ax.set_title('Curva Precision-Recall', fontsize=11, fontweight='bold')
    ax.legend(loc="lower left")
    ax.grid(alpha=0.3)
    
    # Distribui√ß√£o de probabilidades
    ax = fig.add_subplot(gs[2, 4:6])
    epb_probs = y_proba[y_test == 1, 1]
    no_epb_probs = y_proba[y_test == 0, 1]
    
    ax.hist(no_epb_probs, bins=20, alpha=0.6, label='Sem EPB (real)', 
            color='blue', density=True)
    ax.hist(epb_probs, bins=20, alpha=0.6, label='EPB (real)', 
            color='red', density=True)
    ax.axvline(x=0.5, color='green', linestyle='--', linewidth=2,
               label='Threshold (0.5)')
    ax.set_xlabel('Probabilidade de EPB', fontsize=10)
    ax.set_ylabel('Densidade', fontsize=10)
    ax.set_title('Distribui√ß√£o de Probabilidades Preditas', fontsize=11, fontweight='bold')
    ax.legend()
    ax.grid(alpha=0.3)
    
    # ========== ROW 4: INTERPRETA√á√ÉO DAS FEATURES ==========
    # Pega exemplos de cada classe
    epb_idx = np.where(y_test == 1)[0][0]
    no_epb_idx = np.where(y_test == 0)[0][0]
    
    for i, (idx, title_prefix) in enumerate([(epb_idx, 'EPB'), 
                                              (no_epb_idx, 'Sem EPB')]):
        # Imagem original
        ax = fig.add_subplot(gs[3, i*3])
        ax.imshow(X_test[idx], cmap='plasma')
        ax.set_title(f'{title_prefix}: Original', fontsize=10)
        ax.axis('off')
        
        # Mapa de import√¢ncia
        importance_map, reconstructed = system.get_feature_importance(X_test[idx])
        
        ax = fig.add_subplot(gs[3, i*3+1])
        im = ax.imshow(importance_map, cmap='hot')
        ax.set_title(f'{title_prefix}: Import√¢ncia', fontsize=10)
        ax.axis('off')
        plt.colorbar(im, ax=ax, fraction=0.046)
        
        # Reconstru√ß√£o
        ax = fig.add_subplot(gs[3, i*3+2])
        ax.imshow(reconstructed, cmap='plasma')
        ax.set_title(f'{title_prefix}: Reconstru√≠do', fontsize=10)
        ax.axis('off')
    
    # ========== ROW 5: COMPONENTES PRINCIPAIS ==========
    # Visualiza primeiros componentes principais
    n_comp_to_show = min(6, system.n_components)
    for i in range(n_comp_to_show):
        ax = fig.add_subplot(gs[4, i])
        component = system.tdpca.projection_matrix[:, i].reshape(-1, 1)
        # Expande para visualiza√ß√£o
        component_img = np.tile(component, (1, X_test.shape[1]))
        ax.imshow(component_img.T, cmap='RdBu_r', aspect='auto')
        ax.set_title(f'Comp {i+1}\n({variance_ratio[i]:.1%})', fontsize=9)
        ax.axis('off')
    
    plt.suptitle('üåå An√°lise Completa: Sistema de Reconhecimento de EPBs', 
                 fontsize=16, fontweight='bold', y=0.995)
    
    return fig


def explain_feature_importance(system, X_sample, y_sample, filename=None):
    """
    Explica quais regi√µes da imagem contribuem para a classifica√ß√£o
    """
    print("\n" + "="*70)
    print("üîç INTERPRETA√á√ÉO DAS FEATURES")
    print("="*70)
    
    importance_map, reconstructed = system.get_feature_importance(X_sample)
    
    # Predi√ß√£o
    prob = system.predict_proba(X_sample.reshape(1, *X_sample.shape))[0]
    pred = system.predict(X_sample.reshape(1, *X_sample.shape))[0]
    
    print(f"\nüì∏ Imagem: {filename if filename else 'N/A'}")
    print(f"   ‚Ä¢ Classe real: {'EPB' if y_sample == 1 else 'Sem EPB'}")
    print(f"   ‚Ä¢ Predi√ß√£o: {'EPB' if pred == 1 else 'Sem EPB'}")
    print(f"   ‚Ä¢ Probabilidade EPB: {prob[1]*100:.2f}%")
    print(f"   ‚Ä¢ Confian√ßa: {'Alta' if max(prob) > 0.8 else 'M√©dia' if max(prob) > 0.6 else 'Baixa'}")
    
    # Analisa regi√µes importantes
    threshold = np.percentile(importance_map, 90)  # Top 10% pixels
    important_pixels = importance_map > threshold
    
    # Divide imagem em quadrantes
    h, w = importance_map.shape
    quadrants = {
        'Superior Esquerdo': important_pixels[:h//2, :w//2].sum(),
        'Superior Direito': important_pixels[:h//2, w//2:].sum(),
        'Inferior Esquerdo': important_pixels[h//2:, :w//2].sum(),
        'Inferior Direito': important_pixels[h//2:, w//2:].sum()
    }
    
    print(f"\nüìä Regi√µes mais importantes (pixels cr√≠ticos):")
    total = sum(quadrants.values())
    for region, count in sorted(quadrants.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total * 100) if total > 0 else 0
        print(f"   ‚Ä¢ {region}: {count} pixels ({percentage:.1f}%)")
    
    # Interpreta√ß√£o
    print(f"\nüí° Interpreta√ß√£o:")
    if pred == 1:  # EPB detectado
        print("   ‚úì O modelo identificou padr√µes caracter√≠sticos de EPB:")
        print("     - Estruturas verticais/irregulares")
        print("     - Deplec√ß√µes localizadas de plasma")
        print("     - Distribui√ß√£o espacial t√≠pica de bolhas")
    else:  # Sem EPB
        print("   ‚úì O modelo N√ÉO detectou padr√µes de EPB:")
        print("     - Distribui√ß√£o uniforme de plasma")
        print("     - Aus√™ncia de estruturas irregulares")
        print("     - Caracter√≠sticas de ionosfera quieta")
    
    # Visualiza
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    axes[0].imshow(X_sample, cmap='plasma')
    axes[0].set_title('Imagem Original', fontsize=12, fontweight='bold')
    axes[0].axis('off')
    
    im = axes[1].imshow(importance_map, cmap='hot')
    axes[1].set_title('Mapa de Import√¢ncia\n(Regi√µes cr√≠ticas para classifica√ß√£o)', 
                     fontsize=12, fontweight='bold')
    axes[1].axis('off')
    plt.colorbar(im, ax=axes[1], fraction=0.046, label='Import√¢ncia')
    
    axes[2].imshow(reconstructed, cmap='plasma')
    axes[2].set_title('Reconstru√ß√£o 2DPCA\n(Features capturadas)', 
                     fontsize=12, fontweight='bold')
    axes[2].axis('off')
    
    fig.suptitle(f'Interpreta√ß√£o: {"EPB" if y_sample == 1 else "Sem EPB"} '
                f'(Predi√ß√£o: {"EPB" if pred == 1 else "Sem EPB"}, {prob[1]*100:.1f}%)',
                fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    return importance_map
